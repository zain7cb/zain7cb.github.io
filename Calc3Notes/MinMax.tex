\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{physics}
\DeclareMathOperator{\grd}{grad}
\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\declaretheoremstyle[name=Theorem,]{thmsty}
\declaretheorem[style=thmsty,numberwithin=section]{theorem}
\tcolorboxenvironment{theorem}{colback=LightGray}

\declaretheoremstyle[name=Proposition,]{prosty}
\declaretheorem[style=prosty,numberlike=theorem]{proposition}
\tcolorboxenvironment{proposition}{colback=LightOrange}

\declaretheoremstyle[name=Principle,]{prcpsty}
\declaretheorem[style=prcpsty,numberlike=theorem]{principle}
\tcolorboxenvironment{principle}{colback=LightGreen}

\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------
\section*{Critical Points}

A point $P$ is a \textbf{critical point} of $f$ if $\grd f (P) = O$. 
Equivalently, all the partial derivatives $D_i f$ are $0$ at $P$.

\textbf{Example.} 
Find the critical points of $f(x,y) = e^{-(x^2+y^2)}$. We take 
partial derivatives and set them to $0$ to find the critical points.

As in the single variable case, we can have a variety of 
behaviors at a critical point; we do not necessarily have
a local minimum or local maximum.

Let $f$ be defined on an open set $U$. A point $P$ is called a
\textbf{local maximum} of $f$ if, in some
neighborhood $N$ of $P$, we have 
\[f(X) \leq f(P)\]
for all $X \in N$.

The concept of local minimum is defined similarly. 

\textbf{Theorem.} Let $f$ be a differentiable function on $U$.
Let $P$ be a local maximum. Then $P$ is a critical point of $f$.

The proof of this amounts to reducing it to a one variable problem.
If $H$ is a nonzero vector, and $t$ is small enough, then 
$P + tH \in U$. Moreover, if $t$ is small enough, $P+tH$ will land
in the neighborhood mentioned in the definition, so that
\[f(P+tH) \leq f(P)\]
for all $t$ in an interval of the form $(-\delta, \delta)$, $\delta > 0$.
So $g(t) = f(P+tH)$ has a local maximum at $t=0$. Thus $g'(t)=0$. 
By the chain rule, 
\[\grd f (P) \cdot H = 0.\]
This is true for all $H$, so we must have $\grd f (P) = 0$. $\blacksquare$

A similar argument shows that local minima are also critical points 
of $f$.

\section*{Boundary, Interior, etc.}

An \textbf{open ball} of radius $r>0$ in $\mathbb{R}^n$ centered at $P$ is defined to be 
the set of all points $X$ such that $\norm{X-P} < r$. 


A \textbf{closed ball} is similarly defined except $\norm{X-P} \leq r$ (rather than strict inequality).

A subset $U \subseteq \mathbb{R}^n$ is \textbf{open} if at every point $P \in U$, there is a ball of some radius around $P$
contained entirely in $U$.

Note that open balls are indeed open sets.

An \textbf{interior point} $P$ of a set $S \subseteq \mathbb{R}^n$ is one such there exists a ball
of some radius around $P$ contained entirely in $S$. Thus one could rephrase the definition of openness
as each point being an interior point.

A point $P$ (not necessarily in $S$) is called a \textbf{boundary point} of $S$ if every open ball around 
$P$ contains both a point in $S$ and a point not in $S$.

A set is \textbf{closed} if it contains all of its boundary points.

A set is \textbf{bounded} if one can fit the set inside a ball. Equivalently, $S$ is bounded
if there is some $b>0$ such that $\norm{X} \leq b$ for all $X\in S$.

\textbf{Theorem.} Let $f$ be a continuous function defined on a closed and bounded set $S$. Then $f$
has a maximum and a minimum on $S$. 

The proof of this requires things out of the scope of this course.

One can see via examples that a continuous $f$ can fail to achieve extreme values if $S$ is not closed and bounded.
As an easy example, consider $f:(0,1)\to \mathbb{R}$, $f(x)=x$. 

In a general situation for some $f$ on a closed and bounded region $S$, we find the maxima and minima
by looking at the critical points in the interior of $S$. One must also look at the values of $f$ on the boundary,
however.

\textbf{Example.} Find the maxima and minima of $f(x,y) = x^3 + xy$ on the square with vertices
$(\pm 1, \pm 1)$. We do this by finding the critical points on the interior, and then also testing
the boundary points by parametrizing the four sides that make up the square and plugging those
parametrizations into $f$.

\textbf{Example.} Find the maximum of the function
\[f(x,y) = x^2 e^{-x^4-y^2}.\]

This function goes to $0$ as the distance from the origin goes
to $\infty$. This implies that the function indeed achieves a
maximum (as a result of the above theorem).

\section*{Lagrange Multipliers}

Now suppose we are trying to find minima and maxima of a function
$f:\mathbb{R}^n \to \mathbb{R}$, but have the added constraint
that we can only plug in points lying on the curve/surface $g(X)=0$
where $g$ is some other function.

\textbf{Theorem.} Let $f$ and $g$ be differentiable functions on $U$ with
continuous partial derivatives. Let $S$ be the set of points $X$
where $g(X)=0$ and $\grd g(X) \neq O$. Suppose that $P$ is 
an extremum of $f$ on $S$. Then there is some number $\lambda$ such that
\[\grd f(P) = \lambda \grd g(P).\]

To get a sense of why this is true, let $X(t)$ be a 
differentiable curve passing through $P$, say $X(t_0)=P$.
Then $f(X(t))$ has a maximum or minimum at $t_0$. So 
\[\frac{d}{dt} f(X(t)) \bigg\vert_{t_0} =0.\]
But the chain rule implies
\[0=\frac{d}{dt} f(X(t)) \bigg\vert_{t_0} = \grd f(P) \cdot X'(t_0).\]
So $\grd f(P)$ is perpendicular to every curve on the surface
passing through $P$. In other words, $\grd f(P)$ and $\grd g(P)$
are both perpendicular to the surface $g(X)=0$ at the point $P$.
So one is a multiple of the other.

This theorem gives us a way of finding the extrema of $f$ subject to
a constraint. The theorem says that if you have an extreme value,
this equation involving the gradients must hold. So one can solve this equation
to get a list of points that are candidates for the extreme values and then figure out the
max and min by inspection. In other words, the theorem says, 
``If there are any extreme values, this is where they would have to be.''

\textbf{Example.} Find the maximum of the function $f(x,y,)=x+y$
subject to the constraint $x^2 + y^2 = 1$. We have $\grd f = (1,1)$
and $\grd g = (2x,2y)$. So suppose $(x_0,y_0)$ is an extreme value.
The equation $\grd f(x_0,y_0)  = \lambda g(x_0,y_0)$ yields the 
two equations
\[1=2\lambda x_0,\ 1 = 2\lambda y_0.\]
So $x_0$ and $y_0$ are nonzero. Hence $\lambda = 1/(2x_0) = 1/(2y_0)$,
meaning $x_0 = y_0$. This point must also satisfy $x_0^2 + y_0^2 = 1$,
so that gives the possibilities 
\[x_0 = \pm \frac{1}{\sqrt{2}}, y_0 = \pm \frac{1}{\sqrt{2}}.\]
So the two solutions to the system of equations are $(-1/\sqrt{2},-1/\sqrt{2})$
and $(1/\sqrt{2},1/\sqrt{2})$. The maximum obviously corresponds to the
latter, where $f$ has the value $2/\sqrt{2}$.
\end{document}